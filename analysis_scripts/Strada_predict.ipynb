{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used trained model to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (the main logic come from /lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/transformer_based_detection/informers/informerrunner.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "site.getusersitepackages()\n",
    "sys.path.append(site.getusersitepackages())\n",
    "sys.path.append('/lcrc/group/ATLAS/users/jj/DiHydra_jj/transformer_based_detection/informers')\n",
    "sys.path.append('/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/utils')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input for informerrunner.py\n",
    "#comes from the class of informerrunner self.__init__,  make it to global variables to handle\n",
    "checkpoint_dir = \"/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/transformer_based_detection/informers/checkpoints\"\n",
    "loss_type='mse'\n",
    "use_spot_detection=False # for onlince detection only\n",
    "device= 'cpu'\n",
    "output_queue = None\n",
    "setting = '/hlt_dcm_2023_mse_seed_42'\n",
    "param_file_string= checkpoint_dir + setting + '/model_parameters.json' \n",
    "path = checkpoint_dir\n",
    "model_path = path + setting + '/checkpoint_informer.pth'\n",
    "\n",
    "#Don't change following parameters\n",
    "data_x_last = None\n",
    "anomaly_start = None\n",
    "anomaly_duration = 0\n",
    "#nan_output_tolerance_period = {\"count\": 0, \"threshold\": 10}\n",
    "nan_output_tolerance_period = {} # threshold of tolerated consecutive NaN predictions of informer is 10\n",
    "nan_output_count = 0\n",
    "predictions_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(param_file_string, 'r') as parameter_dict_file:\n",
    "    parameter_dict = json.load(parameter_dict_file)\n",
    "parameter_dict['timeenc'] = 0 if parameter_dict['embed'] != 'timeF' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /lcrc/group/ATLAS/users/jj/DiHydra_jj\n",
      "Current working directory: /lcrc/group/ATLAS/users/jj/DiHydra_jj\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir('/lcrc/group/ATLAS/users/jj/DiHydra_jj')\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections.abc import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined')\n",
    "from transformer_based_detection.informers.models.model import Informer\n",
    "from transformer_based_detection.informers.utils.datapreprocessor import DataPreprocessor\n",
    "from detection_combined.utils.exceptions import NonCriticalPredictionException\n",
    "from utils.spot import SPOT\n",
    "from anomalyclassification import AnomalyType\n",
    "#from utils.exceptions import NonCriticalPredictionException\n",
    "#from utils.anomalyclassification import AnomalyType  # Why cannot find Module by this path?\n",
    "from detection_combined.utils.anomalyclassification import AnomalyType\n",
    "from detection_combined.utils.tqdmloggingdecorator import tqdmloggingdecorator\n",
    "\n",
    "data_preprocessor = DataPreprocessor(parameter_dict,checkpoint_dir)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/transformer_based_detection/informers/checkpoints\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    params = parameter_dict\n",
    "    model = Informer(params['enc_in'], params['dec_in'], params['c_out'], params['seq_len'],\n",
    "                     params['label_len'], params['pred_len'], params['factor'], params['d_model'],\n",
    "                     params['n_heads'], params['e_layers'], params['d_layers'], params['d_ff'],\n",
    "                     params['dropout'], params['attn'], params['embed'], params['freq'],\n",
    "                     params['activation'], True, params['distil'], params['mix'], device ).float()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Informer(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(146, 576, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=6, out_features=576, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.001, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(146, 576, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=6, out_features=576, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.001, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.001, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (key_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (value_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (out_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(576, 2944, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2944, 576, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_layers): ModuleList()\n",
       "    (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.001, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (key_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (value_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (out_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.001, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (key_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (value_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "          (out_projection): Linear(in_features=576, out_features=576, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(576, 2944, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2944, 576, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=576, out_features=146, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "#eval() will switch model to \"evaluation mode,\" \n",
    "#but it does not perform the evaluation!\n",
    "#use model without calling eval()), can lead to: \n",
    "#1. Unstable predictions due to dropout layer\n",
    "#2. Performance issues because of incorrect BatchNorm statistics\n",
    "#However, in the informer we use layerNorm instead of BatchNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_one_batch(dataset_object,\n",
    "                        batch_x,\n",
    "                        batch_y,\n",
    "                        batch_x_mark,\n",
    "                        batch_y_mark,\n",
    "                        viz_data):\n",
    "\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        # Decoder input\n",
    "\n",
    "        padding = parameter_dict['padding']\n",
    "        pred_len = parameter_dict['pred_len']\n",
    "        label_len = parameter_dict['label_len']\n",
    "\n",
    "        if padding == 0:\n",
    "            dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\n",
    "\n",
    "        elif padding == 1:\n",
    "            dec_inp = torch.ones([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\n",
    "\n",
    "        dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "\n",
    "        # Encoder - decoder\n",
    "\n",
    "        outputs, _ = model(batch_x, batch_x_mark, dec_inp, batch_y_mark,\n",
    "                                                            viz_data=viz_data)\n",
    "\n",
    "        if parameter_dict['inverse']:\n",
    "            outputs = dataset_object.inverse_transform(outputs)\n",
    "\n",
    "        f_dim = 0\n",
    "        batch_y = batch_y[:, -pred_len:, f_dim:].to(device)\n",
    "\n",
    "        return outputs, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tqdmloggingdecorator\n",
    "def detect(data: pd.DataFrame, \n",
    "           data_preprocessor, \n",
    "           parameter_dict, \n",
    "           nan_output_tolerance_period, \n",
    "           logger, \n",
    "           predictions_all, \n",
    "           data_x_last, \n",
    "           anomaly_start, \n",
    "           anomaly_duration, \n",
    "           detection_callback):\n",
    "\n",
    "    data_x, data_y, data_x_mark, data_y_mark = data_preprocessor.process(data)\n",
    "    timestamp = data.index[-1]\n",
    "\n",
    "    # Convert data to numpy for visualization\n",
    "    viz_data = data.to_numpy()[:parameter_dict['seq_len'], :]\n",
    "\n",
    "    # Process a single batch\n",
    "    preds, _ = _process_one_batch(data_preprocessor, data_x, data_y, data_x_mark, data_y_mark, viz_data)\n",
    "\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    # Check for NaN predictions\n",
    "    if np.any(np.isnan(preds)):\n",
    "        nan_output_count = nan_output_tolerance_period.get(\"count\", 0) + 1\n",
    "        nan_output_tolerance_period[\"count\"] = nan_output_count\n",
    "\n",
    "        if nan_output_count >= nan_output_tolerance_period[\"threshold\"]:\n",
    "            logger.warning('Encountered NaN in Informer predictions')\n",
    "            logger.error(f'Reached threshold of tolerated consecutive NaN predictions of {nan_output_tolerance_period[\"threshold\"]}')\n",
    "            raise NonCriticalPredictionException(\n",
    "                f'Informer reached threshold of tolerated consecutive NaN predictions of {nan_output_tolerance_period[\"threshold\"]}'\n",
    "            )\n",
    "        else:\n",
    "            logger.warning('Encountered NaN in Informer predictions')\n",
    "            logger.warning(f'Consecutive NaN predictions: {nan_output_count}, tolerated NaN predictions: {nan_output_tolerance_period[\"threshold\"]}')\n",
    "        return\n",
    "    else:\n",
    "        nan_output_tolerance_period[\"count\"] = 0\n",
    "\n",
    "    l2_dist_detection = False #Only if current time step l2_dist <0.5\n",
    "\n",
    "    # Calculate L2 distance if data_x_last exists\n",
    "    if data_x_last is not None:\n",
    "        l2_dist = np.mean((preds[:, 0, :] - data_y.detach().cpu().numpy()[:, -1, :])**2, 1)[0]\n",
    "        #l2_dist measure the Euclidean distance between the prediction and the actual real target\n",
    "        predictions_all.append(l2_dist)\n",
    "\n",
    "        l2_dist_detection = (l2_dist > 0.5)\n",
    "\n",
    "        if l2_dist_detection:\n",
    "            if anomaly_duration == 0:\n",
    "                anomaly_start = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                logger.warning(f'Transformer-based detection encountered anomaly at timestamp {anomaly_start}')\n",
    "\n",
    "            detection_callback(0, AnomalyType.TransformerBased, anomaly_start, anomaly_duration)\n",
    "            anomaly_duration += 1\n",
    "        else:\n",
    "            if anomaly_duration != 0:\n",
    "                anomaly_end = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                logger.warning(f'Transformer-based detection anomaly ended at {anomaly_end}')\n",
    "            anomaly_duration = 0\n",
    "\n",
    "    # Update data_x_last for next detection\n",
    "    data_x_last = data_y.detach().cpu().numpy() #pass current timestep real data（data_y）to data_x_last for next call of detect\n",
    "\n",
    "    return predictions_all, data_x_last, anomaly_start, anomaly_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions():\n",
    "    return np.array(predictions_all)#just convert list to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_detection_callback(callback: Callable) -> None:\n",
    "    detection_callback = callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer_based_detection.informers.informerrunner import InformerRunner\n",
    "#informer_runner = InformerRunner(checkpoint_dir, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using above defined functions to make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following logic come from /lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/offline_detection/informers_localData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data_name='/lcrc/group/ATLAS/users/jhoya/DAQ/atlas-hlt-datasets/test_set_dcm_rates_2023.csv'\n",
    "hlt_data_pd = pd.read_csv(inp_data_name, index_col=0, parse_dates=True)\n",
    "hlt_data_np = hlt_data_pd.to_numpy()\n",
    "timestamps = list(hlt_data_pd.index)\n",
    "tpu_labels = list(hlt_data_pd.columns.values)\n",
    "output_dir='/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/transformer_based_detection/informers/JJ_trained_model_predict_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reduceddatabuffer import ReducedDataBuffer \n",
    "reduced_data_buffer = ReducedDataBuffer(size=17) #only for MSE loss, size=65 for SMSE\n",
    "\n",
    "from detection_combined.reduction.medianstdreducer import MedianStdReducer\n",
    "median_std_reducer = MedianStdReducer('2023')\n",
    "sys.path.append('/lcrc/group/ATLAS/users/jj/DiHydra_jj/detection_combined/reduction')\n",
    "from detection_combined.utils.anomalyclassification import AnomalyType\n",
    "from detection_combined.utils.anomalyregistry import JSONAnomalyRegistry\n",
    "json_anomaly_registry = JSONAnomalyRegistry(output_dir)\n",
    "register_detection_callback(json_anomaly_registry.transformer_detection)\n",
    "detection_callback =json_anomaly_registry.transformer_detection\n",
    "\n",
    "reduced_data_buffer.set_buffer_filled_callback(\n",
    "    lambda data: detect(\n",
    "        data,\n",
    "        data_preprocessor,\n",
    "        parameter_dict,\n",
    "        nan_output_tolerance_period,\n",
    "        logger,\n",
    "        predictions_all,\n",
    "        data_x_last,\n",
    "        anomaly_start,\n",
    "        anomaly_duration,\n",
    "        detection_callback,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629b1b48d51c4cb593df70175705d469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24018 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "with logging_redirect_tqdm():\n",
    "    for count, (timestamp, data) in enumerate(tzip(timestamps, hlt_data_np)):\n",
    "        try:\n",
    "            output_slice =\\\n",
    "                median_std_reducer.reduce_numpy(tpu_labels,\n",
    "                                                timestamp,\n",
    "                                                data)\n",
    "            reduced_data_buffer.push(output_slice)\n",
    "\n",
    "        except NonCriticalPredictionException:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
