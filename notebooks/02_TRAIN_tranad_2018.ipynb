{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03ddc7b",
   "metadata": {},
   "source": [
    "# Training the Transformers with 2018 data\n",
    "\n",
    "Following the \"main.py\" script in tranad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7eac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 10:22:06.435747: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 10:22:06.750200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append('/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/transformer_based_detection/tranad/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#from src.models import *\n",
    "# from src.constants import *\n",
    "# from src.plotting import *\n",
    "# from src.pot import *\n",
    "#from src.utils import *\n",
    "# from src.diagnosis import *\n",
    "# from src.merlin import *\n",
    "from src.dataset_loader_hlt_datasets import HLTDataset\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from torch_profiling_utils.torchinfowriter import TorchinfoWriter\n",
    "#from torch_profiling_utils.fvcorewriter import FVCoreWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beba9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    HEADER = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    RED = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b30500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset):\n",
    "\n",
    "    loader = []\n",
    "\n",
    "    if 'HLT' in dataset:\n",
    "\n",
    "        data_source = dataset.split('_')[0]\n",
    "        variant = int(dataset.split('_')[-1])\n",
    "        print(data_source)\n",
    "        print(variant)\n",
    "\n",
    "        train_set = HLTDataset(data_source, variant,\n",
    "                                        'train', False,\n",
    "                                        'minmax', 'train_set_fit',\n",
    "                                        applied_augmentations=\\\n",
    "                                                args.augmentations,\n",
    "                                        augmented_dataset_size_relative=\\\n",
    "                                                args.augmented_dataset_size_relative,\n",
    "                                        augmented_data_ratio=\\\n",
    "                                                args.augmented_data_ratio)\n",
    "\n",
    "        folder = f'./checkpoints/{args.model}_{args.dataset}_{augmentation_string}_seed_{int(args.seed)}/'\n",
    "\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        train_set.pickle_scaler(f'{folder}/scaler.pkl')\n",
    "\n",
    "        loader.append(train_set.get_data())\n",
    "\n",
    "        test_set = HLTDataset(data_source, variant,\n",
    "                                        'test', False,\n",
    "                                        'minmax', 'train_set_fit',\n",
    "                                        applied_augmentations=\\\n",
    "                                                args.augmentations,\n",
    "                                        augmented_dataset_size_relative=\\\n",
    "                                                args.augmented_dataset_size_relative,\n",
    "                                        augmented_data_ratio=\\\n",
    "                                                args.augmented_data_ratio)\n",
    "\n",
    "        loader.append(test_set.get_data())\n",
    "        loader.append(test_set.get_labels())\n",
    "        \n",
    "    #if args.less:\n",
    "    #    loader[0] = cut_array(0.2, loader[0])\n",
    "\n",
    "    print(f'Train shape: {loader[0].shape}')\n",
    "    print(f'Test shape: {loader[1].shape}')\n",
    "\n",
    "    train_loader = DataLoader(loader[0], batch_size=loader[0].shape[0])\n",
    "    test_loader = DataLoader(loader[1], batch_size=loader[1].shape[0])\n",
    "    labels = loader[2]\n",
    "\n",
    "    return train_loader, test_loader, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103c5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, accuracy_list):\n",
    "    folder = f'checkpoints/{args.model}_{args.dataset}_{augmentation_string}_seed_{int(args.seed)}/'\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = f'{folder}/model.ckpt'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'accuracy_list': accuracy_list}, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adf3ed",
   "metadata": {},
   "source": [
    "# DAGMM (Deep Autoencoding Gaussian Mixture Model)\n",
    "\n",
    "1. Autoencoder: Compresses the input data into a lower-dimensional latent representation.\n",
    "1. Reconstruction Error Metrics: Computes reconstruction errors (e.g., relative Euclidean distance and cosine similarity) between the original and reconstructed data.\n",
    "1. Estimation Network (GMM): Takes the latent representation and reconstruction errors to estimate the parameters of a GMM.\n",
    "1. Energy Function: Computes the energy (negative log-likelihood) of the samples under the estimated GMM.\n",
    "1. Loss Function: Combines the reconstruction loss, the sample energy, and a regularization term on the covariance matrices.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- An autoencoder with an estimation network for GMM parameters. Reconstruction errors are used to enhance the latent representation.\n",
    "- Combines reconstruction loss, sample energy, and covariance regularization.\n",
    "- Windowing is essential for time series data. Flattened windows are used as inputs to the model.\n",
    "- Iterate over the data, compute the loss, and update the model parameters.\n",
    "- During testing, compute the sample energy as the anomaly score. Analyze the energy scores to identify anomalies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32367227",
   "metadata": {},
   "source": [
    "1. Autoencoder\n",
    "\n",
    "    - Dimensionality Reduction: Compresses input data into a latent space.\n",
    "    - Feature Learning: Captures essential patterns and structures in the data.\n",
    "    - Reconstruction: Attempts to reconstruct the input from the latent representation.\n",
    "\n",
    "How it works:\n",
    "\n",
    "- Encoder: Maps input data $\\mathbf{x}$ to a latent representation $\\mathbf{z}$:  \n",
    "\n",
    "$$\n",
    "\\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}; \\theta_{\\text{enc}})\n",
    "$$\n",
    "\n",
    "- Decoder: Reconstructs the input from $\\mathbf{z}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = f_{\\text{dec}}(\\mathbf{z};\\, \\theta_{\\text{dec}})\n",
    "$$\n",
    "\n",
    "- Training Objective: Minimize the reconstruction loss:\n",
    "\n",
    "$$\n",
    "L_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left\\| \\mathbf{x}_i - \\hat{\\mathbf{x}}_i \\right\\|^2\n",
    "$$\n",
    "\n",
    "The autoencoder is trained primarily on normal (non-anomalous) data, learning to reconstruct these patterns accurately. Unseen or rare patterns (anomalies) are reconstructed poorly, resulting in higher reconstruction errors.\n",
    "\n",
    "2. Reconstruction Error Metrics\n",
    "\n",
    "Reconstruction errors provide cues about how well the model represents each data point. Combining reconstruction errors with latent representations enhances the model's ability to detect anomalies.\n",
    "\n",
    "   1. **Relative Euclidean Distance (RED)**:\n",
    "\n",
    "   $$\n",
    "   \\text{RED} = \\frac{\\left\\| \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\|_2}{\\left\\| \\mathbf{x} \\right\\|_2}\n",
    "   $$\n",
    "\n",
    "   2. **Cosine Similarity (CS)**:\n",
    "\n",
    "   $$\n",
    "   \\text{CS} = \\frac{\\mathbf{x}^\\top \\hat{\\mathbf{x}}}{\\left\\| \\mathbf{x} \\right\\|_2 \\left\\| \\hat{\\mathbf{x}} \\right\\|_2}\n",
    "   $$\n",
    "\n",
    "Higher RED: Indicates poor reconstruction, potential anomaly.\n",
    "Lower CS: Indicates dissimilarity between input and reconstruction.\n",
    "\n",
    "\n",
    "3. Estimation Network and Gaussian Mixture Model (GMM)\n",
    "\n",
    "Density Estimation: Models the distribution of latent representations and reconstruction errors.\n",
    "Anomaly Scoring: Identifies data points that deviate from the learned distribution.\n",
    "\n",
    "- **Input to Estimation Network**:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{v} = \\left[ \\mathbf{z},\\, \\text{RED},\\, \\text{CS} \\right]\n",
    "  $$\n",
    "\n",
    "  - Concatenation of latent representation and reconstruction error metrics.\n",
    "\n",
    "- **Estimation Network**: Outputs mixture component assignments (soft clustering):\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{\\gamma} = f_{\\text{est}}(\\mathbf{v};\\, \\theta_{\\text{est}})\n",
    "  $$\n",
    "\n",
    "  - $\\boldsymbol{\\gamma} \\in \\mathbb{R}^K$, where $K$ is the number of Gaussian components.\n",
    "  - Activation function: Softmax to ensure $\\sum_{k=1}^{K} \\gamma_{ik} = 1$ for each data point $i$.\n",
    "\n",
    "- **GMM Parameters Estimation**:\n",
    "\n",
    "  - **Mixture Weights**:\n",
    "\n",
    "    $$\n",
    "    \\phi_k = \\frac{1}{N} \\sum_{i=1}^{N} \\gamma_{ik}\n",
    "    $$\n",
    "\n",
    "  - **Means**:\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\mu}_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} \\mathbf{v}_i}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "    $$\n",
    "\n",
    "  - **Covariances**:\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\Sigma}_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} \\left( \\mathbf{v}_i - \\boldsymbol{\\mu}_k \\right) \\left( \\mathbf{v}_i - \\boldsymbol{\\mu}_k \\right)^\\top}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "    $$\n",
    "\n",
    "#### Energy Function (Anomaly Score)\n",
    "\n",
    "  $$\n",
    "  E(\\mathbf{v}_i) = -\\log \\left( \\sum_{k=1}^{K} \\phi_k \\cdot \\mathcal{N} \\left( \\mathbf{v}_i \\mid \\boldsymbol{\\mu}_k,\\, \\boldsymbol{\\Sigma}_k \\right) \\right)\n",
    "  $$\n",
    "\n",
    "  - $\\mathcal{N}(\\mathbf{v}_i \\mid \\boldsymbol{\\mu}_k,\\, \\boldsymbol{\\Sigma}_k)$ is the Gaussian probability density function for component $k$.\n",
    "\n",
    "  - **Low Energy**: Data point is likely under the model (considered normal).\n",
    "  - **High Energy**: Data point is unlikely under the model (potential anomaly).\n",
    "\n",
    "\n",
    "4. Joint Optimization\n",
    "\n",
    "The total loss function combines:\n",
    "\n",
    "1. **Reconstruction Loss**:\n",
    "\n",
    "   $$\n",
    "   L_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left\\| \\mathbf{x}_i - \\hat{\\mathbf{x}}_i \\right\\|^2\n",
    "   $$\n",
    "\n",
    "2. **Sample Energy**:\n",
    "\n",
    "   $$\n",
    "   L_{\\text{energy}} = \\frac{1}{N} \\sum_{i=1}^{N} E(\\mathbf{v}_i)\n",
    "   $$\n",
    "\n",
    "3. **Covariance Regularization**:\n",
    "\n",
    "   $$\n",
    "   L_{\\text{cov}} = \\sum_{k=1}^{K} \\frac{1}{\\left\\| \\boldsymbol{\\Sigma}_k \\right\\|_F^2}\n",
    "   $$\n",
    "\n",
    "   - $\\left\\| \\cdot \\right\\|_F$ denotes the Frobenius norm.\n",
    "\n",
    "#### Total Loss Function\n",
    "\n",
    "The overall objective is to minimize:\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{recon}} + \\lambda_{\\text{energy}} \\cdot L_{\\text{energy}} + \\lambda_{\\text{cov}} \\cdot L_{\\text{cov}}\n",
    "$$\n",
    "\n",
    "- $\\lambda_{\\text{energy}}$, $\\lambda_{\\text{cov}}$: Regularization coefficients controlling the importance of each term.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "- **Joint Training**: The autoencoder and estimation network are trained simultaneously to optimize the total loss.\n",
    "- **Backpropagation**: Gradients are computed for all components, and parameters are updated using optimization algorithms.\n",
    "- **Benefit**: Ensures that the latent space is structured in a way that is conducive to density estimation and effective anomaly detection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8f5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DAGMM Model (ICLR 18)\n",
    "\n",
    "#self.n_feats: Number of features in the dataset.\n",
    "#self.n_hidden: Size of the hidden layers.\n",
    "#self.n_latent: Size of the latent representation.\n",
    "#self.n_window: Window size for time series data (since we're dealing with sequences).\n",
    "#self.n: Total input size after windowing (n_feats * n_window).\n",
    "#self.n_gmm: Number of Gaussian mixture components (set equal to n).\n",
    "                                                 \n",
    "class DAGMM(nn.Module):\n",
    "    def __init__(self, feats):\n",
    "        super(DAGMM, self).__init__()\n",
    "        self.name = 'DAGMM'\n",
    "        self.lr = 0.0001\n",
    "        self.beta = 0.01\n",
    "        self.n_feats = feats\n",
    "        self.n_hidden = 16\n",
    "        self.n_latent = 8\n",
    "        self.n_window = 5 # DAGMM w_size = 5\n",
    "        self.n = self.n_feats * self.n_window\n",
    "        self.n_gmm = self.n_feats * self.n_window\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n, self.n_hidden), nn.Tanh(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden), nn.Tanh(),\n",
    "            nn.Linear(self.n_hidden, self.n_latent)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.n_latent, self.n_hidden), nn.Tanh(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden), nn.Tanh(),\n",
    "            nn.Linear(self.n_hidden, self.n), nn.Sigmoid(),\n",
    "        )\n",
    "        # Estimation network:\n",
    "        #Input: Concatenation of latent representation and 2 reconstruction error metrics.\n",
    "        #Output: Gamma (mixture component probabilities).\n",
    "        #Activation: Tanh, followed by Softmax to ensure the output sums to 1.\n",
    "        self.estimate = nn.Sequential(\n",
    "            nn.Linear(self.n_latent+2, self.n_hidden), nn.Tanh(), nn.Dropout(0.5),\n",
    "            nn.Linear(self.n_hidden, self.n_gmm), nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def compute_reconstruction(self, x, x_hat):\n",
    "        # Reconstruction Error Metrics\n",
    "        #Relative Euclidean Distance: Measures the reconstruction error \n",
    "        #                             relative to the norm of the original input.\n",
    "        # Cosine Similarity: Measures the angle between the original and \n",
    "        #                    reconstructed input vectors.\n",
    "        relative_euclidean_distance = (x-x_hat).norm(2, dim=1) / x.norm(2, dim=1)\n",
    "        cosine_similarity = F.cosine_similarity(x, x_hat, dim=1)\n",
    "        return relative_euclidean_distance, cosine_similarity\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Encode Decoder\n",
    "        x = x.view(1, -1) # Flatten the input\n",
    "        z_c = self.encoder(x) # Encode to get latent representation z_c\n",
    "        x_hat = self.decoder(z_c) # Decode to reconstruct x_hat\n",
    "        ## Compute Reconstruction errors\n",
    "        rec_1, rec_2 = self.compute_reconstruction(x, x_hat)\n",
    "        z = torch.cat([z_c, rec_1.unsqueeze(-1), rec_2.unsqueeze(-1)], dim=1) # Concatenate z_c with reconstruction errors to form z.\n",
    "        ## Estimate gamma\n",
    "        gamma = self.estimate(z) # Pass z through the estimation network to get gamma.\n",
    "        return z_c, x_hat.view(-1), z, gamma.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d3ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequential data into windowed samples.\n",
    "def convert_to_windows(data, model):\n",
    "    windows = []; w_size = model.n_window\n",
    "    for i, g in enumerate(data): \n",
    "        if i >= w_size: w = data[i-w_size:i]\n",
    "        # If there aren't enough previous steps, pad with the first data point.\n",
    "        else: w = torch.cat([data[0].repeat(w_size-i, 1), data[0:i]])\n",
    "        windows.append(w if 'TranAD' in args.model or 'Attention' in args.model else w.view(-1))\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def load_model(modelname, dims):\n",
    "    #import src.models\n",
    "    #model_class = getattr(src.models, modelname)\n",
    "    model_class = globals().get(modelname)\n",
    "    model = model_class(dims).double()\n",
    "    optimizer = torch.optim.AdamW(model.parameters() , lr=model.lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.9)\n",
    "    fname = f'checkpoints/{args.model}_{args.dataset}_{augmentation_string}_seed_{int(args.seed)}/model.ckpt'\n",
    "\n",
    "    if os.path.exists(fname) and (not args.retrain or args.test):\n",
    "\n",
    "        print(f\"{color.GREEN}Loading pre-trained model: {model.name}{color.ENDC}\")\n",
    "        checkpoint = torch.load(fname)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        accuracy_list = checkpoint['accuracy_list']\n",
    "\n",
    "    else:\n",
    "        print(f\"{color.GREEN}Creating new model: {model.name}{color.ENDC}\")\n",
    "        epoch = -1; accuracy_list = []\n",
    "    return model, optimizer, scheduler, epoch, accuracy_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2c2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args_dict = {\n",
    "    \"use_gpu\": False,\n",
    "    \"use_multi_gpu\": False,\n",
    "    \"seed\":42, \n",
    "    \"dataset\": \"HLT_2018\",\n",
    "    #\"dataset\": \"HLT_2023\",\n",
    "    #\"dataset\": \"HLT_DCM_2023\",\n",
    "    \"checkpoints\": \"./checkpoints\",\n",
    "    \"loss\": \"MSE\",\n",
    "    \"learning_rate\": 0.00009727998365755187,\n",
    "    \"num_workers\": 8, # Before was 0\n",
    "    \"train_epochs\": 10,\n",
    "    \"batch_size\": 256, # Before was 128\n",
    "    \"patience\": 3,\n",
    "    \"model\": \"DAGMM\",\n",
    "    \"retrain\": True,\n",
    "    #\"retrain\": False,\n",
    "    \"test\": False,\n",
    "    #\"test\": True,\n",
    "    \"apply_augmentations\": True,\n",
    "    #\"apply_augmentations\": False,\n",
    "    #\"augmentations\": ['Scale:0.8,1.0', 'Scale_APP:0.8,1.0,0.01,0.05,0.05'],\n",
    "    \"augmentations\": ['Scale:1.0,1.2', 'Scale_APP:1.0,1.2,0.01,0.01,0.01'], \n",
    "    #\"augmentations\": [],\n",
    "    \"augmented_dataset_size_relative\": 1.0,\n",
    "    \"augmented_data_ratio\": 0.25\n",
    "    #\"augmented_data_ratio\": 0\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an argparse.Namespace object\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a22980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale_1.0_1.2_Scale_APP_1.0_1.2_0.01_0.01_0.01_rel_size_1.0_ratio_0.25\n",
      "[('Scale', [1.0, 1.2]), ('Scale_APP', [1.0, 1.2, 0.01, 0.01, 0.01])]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "\n",
    "# We're doing no augmentations for the moment!\n",
    "augmentations = []\n",
    "\n",
    "if args.apply_augmentations:\n",
    "\n",
    "    augmentation_string = ''\n",
    "\n",
    "    for augmentation in args.augmentations:\n",
    "        augmentation = augmentation.replace(' ', '')\n",
    "        aug_type, factors = augmentation.split(':')\n",
    "\n",
    "        factors = factors.split(',')\n",
    "\n",
    "        factors_string = '_'.join(factors)\n",
    "\n",
    "        factors = [float(factor) for factor in factors]\n",
    "\n",
    "        if len(augmentation_string):\n",
    "            augmentation_string += '_'\n",
    "\n",
    "        augmentation_string += aug_type + '_' +\\\n",
    "                                    factors_string\n",
    "\n",
    "        augmentations.append((aug_type, factors))\n",
    "\n",
    "    augmentation_string += f'_rel_size_{args.augmented_dataset_size_relative}'\\\n",
    "                                        f'_ratio_{args.augmented_data_ratio:.2f}'\n",
    "\n",
    "\n",
    "if args.augmented_data_ratio == 0:\n",
    "    augmentation_string = 'no_augment'\n",
    "\n",
    "args.augmentations = augmentations\n",
    "\n",
    "print(augmentation_string)\n",
    "print(augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e01d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLT\n",
      "2018\n",
      "Train shape: (58111, 102)\n",
      "Test shape: (27955, 102)\n",
      "(27955, 102) 102\n",
      "\u001b[92mCreating new model: DAGMM\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DAGMM(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=510, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=16, out_features=510, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       "  (estimate): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=16, out_features=510, bias=True)\n",
       "    (4): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "train_loader, test_loader, labels = load_dataset(args.dataset)\n",
    "#train_loader, test_loader, labels = load_dataset_indiv(args.dataset)\n",
    "print(labels.shape, labels.shape[1])\n",
    "model, optimizer, scheduler, epoch, accuracy_list = load_model(args.model, labels.shape[1])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "trainD, testD = next(iter(train_loader)), next(iter(test_loader))\n",
    "trainO, testO = trainD, testD\n",
    "if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN'] or 'TranAD' in model.name: \n",
    "    trainD, testD = convert_to_windows(trainD, model), convert_to_windows(testD, model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb38901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8003be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(epoch,                   # Current epoch number.\n",
    "                model,                #\n",
    "                data,                 # Windowed data.\n",
    "                dataO,                # Original data (used for feature size).\n",
    "                optimizer,            # \n",
    "                scheduler,            # Learning rate scheduler.\n",
    "                training=True,        # Training or Validation?\n",
    "                summary_writer=None,  # For logging\n",
    "                dataset_name=None):    \n",
    "\n",
    "    # Loss Function\n",
    "    # Training: Uses mean reduction to compute the average loss.\n",
    "    # Validation: Uses 'none' reduction to compute the loss per sample.\n",
    "    l = nn.MSELoss(reduction = 'mean' if training else 'none')\n",
    "    feats = dataO.shape[1]\n",
    "\n",
    "    data_train_list = []\n",
    "    preds_train_list = []\n",
    "    data_test_list = []\n",
    "    preds_test_list = []\n",
    "\n",
    "\n",
    "    if 'DAGMM' in model.name:\n",
    "        l = nn.MSELoss(reduction = 'none')\n",
    "        # To compute the energy and covariance regularization terms.\n",
    "        compute = ComputeLoss(model, 0.1, 0.005, device, model.n_gmm)\n",
    "        n = epoch + 1; w_size = model.n_window\n",
    "        l1s = []; l2s = []\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "#         if training:\n",
    "#             for d in data:\n",
    "#                 d = d.to(device)\n",
    "#                 _, x_hat, z, gamma = model(d)\n",
    "#                 l1, l2 = l(x_hat, d), l(gamma, d)\n",
    "#                 l1s.append(torch.mean(l1).item()); l2s.append(torch.mean(l2).item())\n",
    "#                 loss = torch.mean(l1) + torch.mean(l2)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#             #print(l1s, l2s)\n",
    "#             scheduler.step()\n",
    "#             tqdm.write(f'Epoch {epoch},\\tL1 = {np.mean(l1s)},\\tL2 = {np.mean(l2s)}')\n",
    "#             return np.mean(l1s)+np.mean(l2s), optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if training:\n",
    "            total_loss = 0\n",
    "            for d in data:\n",
    "                d = d.to(device)\n",
    "                z_c, x_hat, z, gamma = model(d)\n",
    "                loss = compute.forward(d.view(1, -1), x_hat.view(1, -1), z, gamma.view(1, -1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            avg_loss = total_loss / len(data)\n",
    "            tqdm.write(f'Epoch {epoch},\\tLoss = {avg_loss}')\n",
    "            return avg_loss, optimizer.param_groups[0]['lr']\n",
    "        \n",
    "#         else:\n",
    "#             ae1s = []\n",
    "\n",
    "#             for d in data:\n",
    "#                 # d = d.to(device)\n",
    "#                 _, x_hat, _, _ = model(d)\n",
    "#                 ae1s.append(x_hat)\n",
    "#             ae1s = torch.stack(ae1s)\n",
    "#             y_pred = ae1s[:, data.shape[1]-feats:data.shape[1]].view(-1, feats)\n",
    "#             loss = l(ae1s, data)[:, data.shape[1]-feats:data.shape[1]].view(-1, feats)\n",
    "#             return loss.detach().cpu().numpy(), y_pred.detach().cpu().numpy()\n",
    "                       \n",
    "                       \n",
    "        else:\n",
    "            anomaly_scores = []\n",
    "            for d in data:\n",
    "                d = d.to(device)\n",
    "                z_c, x_hat, z, gamma = model(d)\n",
    "                sample_energy, _ = compute.compute_energy(z, gamma)\n",
    "                anomaly_scores.append(sample_energy.item())\n",
    "            return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcc455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_numpy_array(array: np.array, filename: str):\n",
    "    \n",
    "    # Create all necessary directories in the path if they don't exist\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open(filename, 'wb') as output_file:\n",
    "        np.save(output_file, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac91bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracy_list, folder):\n",
    "    os.makedirs(f'plots/{folder}/', exist_ok=True)\n",
    "    trainAcc = [i[0] for i in accuracy_list]\n",
    "    lrs = [i[1] for i in accuracy_list]\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Average Training Loss')\n",
    "    plt.plot(range(len(trainAcc)), trainAcc, label='Average Training Loss', linewidth=1, linestyle='-', marker='.')\n",
    "    plt.twinx()\n",
    "    plt.plot(range(len(lrs)), lrs, label='Learning Rate', color='r', linewidth=1, linestyle='--', marker='.')\n",
    "    plt.savefig(f'plots/{folder}/training-graph.pdf')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b6b77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the loss function as per the DAGMM paper, including:\n",
    "#\n",
    "#    Reconstruction Loss: Between the original input x and the reconstructed output x_hat.\n",
    "#    Sample Energy: Negative log-likelihood under the GMM.\n",
    "#    Covariance Regularization: Encourages the covariance matrices to be well-conditioned.\n",
    "from torch.autograd import Variable\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, lambda_energy, lambda_cov, device, n_gmm):\n",
    "        self.model = model\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov = lambda_cov\n",
    "        self.device = device\n",
    "        self.n_gmm = n_gmm\n",
    "    \n",
    "    # Cumputes the total loss and returns a scalar loss tensor for backpropagation\n",
    "    def forward(self, x, x_hat, z, gamma):\n",
    "        \"\"\"Computing the loss function for DAGMM.\"\"\"\n",
    "        reconst_loss = torch.mean((x-x_hat).pow(2))\n",
    "\n",
    "        sample_energy, cov_diag = self.compute_energy(z, gamma)\n",
    "\n",
    "        loss = reconst_loss + self.lambda_energy * sample_energy + self.lambda_cov * cov_diag\n",
    "        #return Variable(loss, requires_grad=True)\n",
    "        return loss\n",
    "    \n",
    "#     def compute_energy(self, z, gamma, phi=None, mu=None, cov=None, sample_mean=True):\n",
    "#         \"\"\"Computing the sample energy function\"\"\"\n",
    "#         if (phi is None) or (mu is None) or (cov is None):\n",
    "#             phi, mu, cov = self.compute_params(z, gamma)\n",
    "\n",
    "#         eps = 1e-12\n",
    "#         K, D = cov.shape[0], cov.shape[1]\n",
    "\n",
    "#         # Add small value to the diagonal for numerical stability\n",
    "#         cov += eps * torch.eye(D, device=self.device).unsqueeze(0)\n",
    "\n",
    "#         # Compute the inverse and determinant of covariance matrices using batch operations\n",
    "#         cov_inverse = torch.inverse(cov)  # Shape: K x D x D\n",
    "\n",
    "#         # Use custom Cholesky function in batch mode\n",
    "#         L = torch.cholesky(cov * (2 * np.pi))  # Shape: K x D x D\n",
    "#         det_cov = torch.prod(torch.diagonal(L, dim1=-2, dim2=-1), dim=1) ** 2  # Shape: K\n",
    "\n",
    "#         # Compute Mahalanobis distance\n",
    "#         z_mu = z.unsqueeze(1) - mu.unsqueeze(0)  # Shape: N x K x D\n",
    "#         z_mu = z_mu.unsqueeze(-1)  # Shape: N x K x D x 1\n",
    "\n",
    "#         # Expand cov_inverse to match z_mu dimensions\n",
    "#         cov_inverse_expanded = cov_inverse.unsqueeze(0)  # Shape: 1 x K x D x D\n",
    "\n",
    "#         # Compute (z - mu)^T * cov_inverse * (z - mu)\n",
    "#         mahal = torch.matmul(cov_inverse_expanded, z_mu).squeeze(-1)  # Shape: N x K x D\n",
    "#         mahal = torch.sum(z_mu.squeeze(-1) * mahal, dim=-1)  # Shape: N x K\n",
    "\n",
    "#         # Compute the Gaussian likelihood\n",
    "#         exp_term = torch.exp(-0.5 * mahal)  # Shape: N x K\n",
    "#         denom = torch.sqrt(det_cov + eps)  # Shape: K\n",
    "#         phi = phi.unsqueeze(0)  # Shape: 1 x K\n",
    "#         probs = phi * exp_term / denom.unsqueeze(0)  # Shape: N x K\n",
    "\n",
    "#         # Compute energy\n",
    "#         E_z = -torch.log(torch.sum(probs, dim=1) + eps)  # Shape: N\n",
    "\n",
    "#         if sample_mean:\n",
    "#             E_z = torch.mean(E_z)\n",
    "#         cov_diag = torch.sum(1 / torch.diagonal(cov, dim1=-2, dim2=-1))\n",
    "\n",
    "#         return E_z, cov_diag\n",
    "\n",
    "    # work with diagonal covariance matrices, avoiding matrix inversions and determinants to speed up.\n",
    "    def compute_energy(self, z, gamma, phi=None, mu=None, cov_diag=None, sample_mean=True):\n",
    "        \"\"\"Computing the sample energy function with diagonal covariance matrices\"\"\"\n",
    "        if (phi is None) or (mu is None) or (cov_diag is None):\n",
    "            phi, mu, cov_diag = self.compute_params(z, gamma)\n",
    "\n",
    "        eps = 1e-12\n",
    "        K, D = cov_diag.shape\n",
    "\n",
    "        # Add small value to the covariance diagonal for numerical stability\n",
    "        cov_diag += eps\n",
    "\n",
    "        # Compute the log determinant of diagonal covariance matrices\n",
    "        log_det_cov = torch.sum(torch.log(cov_diag), dim=1)  # Shape: K\n",
    "\n",
    "        # Compute Mahalanobis distance for diagonal covariance\n",
    "        z_mu = z.unsqueeze(1) - mu.unsqueeze(0)  # Shape: N x K x D\n",
    "        mahal = (z_mu ** 2) / cov_diag.unsqueeze(0)  # Shape: N x K x D\n",
    "        mahal = torch.sum(mahal, dim=2)  # Shape: N x K\n",
    "\n",
    "        # Precompute D * log(2 * π)\n",
    "        D_log_2pi = D * np.log(2 * np.pi)\n",
    "        D_log_2pi = torch.tensor(D_log_2pi, device=self.device)\n",
    "\n",
    "        # Compute the log probability density function\n",
    "        log_prob = -0.5 * (mahal + log_det_cov.unsqueeze(0) + D_log_2pi)\n",
    "\n",
    "        # Compute the weighted log probabilities\n",
    "        weighted_log_prob = torch.log(phi.unsqueeze(0) + eps) + log_prob  # Shape: N x K\n",
    "\n",
    "        # Compute log-sum-exp for numerical stability\n",
    "        log_sum_exp = torch.logsumexp(weighted_log_prob, dim=1)  # Shape: N\n",
    "\n",
    "        # Compute the negative log-likelihood (energy)\n",
    "        E_z = -log_sum_exp  # Shape: N\n",
    "\n",
    "        if sample_mean:\n",
    "            E_z = torch.mean(E_z)\n",
    "        cov_diag_sum = torch.sum(1 / cov_diag)\n",
    "        return E_z, cov_diag_sum\n",
    "\n",
    "\n",
    "\n",
    "#     def compute_params(self, z, gamma):\n",
    "#         \"\"\"Computing the parameters phi, mu and gamma for sample energy function\"\"\"\n",
    "#         N, D = z.shape\n",
    "#         K = gamma.shape[1]\n",
    "\n",
    "#         # Compute phi (mixture weights): Shape K\n",
    "#         phi = torch.sum(gamma, dim=0) / N  # Shape: K\n",
    "\n",
    "#         # Compute mu (component means): Shape K x D\n",
    "#         gamma_sum = torch.sum(gamma, dim=0)  # Shape: K\n",
    "#         mu = torch.matmul(gamma.t(), z) / gamma_sum.unsqueeze(1)  # Shape: K x D\n",
    "\n",
    "#         # Compute covariance matrices: Shape K x D x D\n",
    "#         z_mu = z.unsqueeze(1) - mu.unsqueeze(0)  # Shape: N x K x D\n",
    "#         gamma_expanded = gamma.unsqueeze(2)  # Shape: N x K x 1\n",
    "#         z_mu = z_mu * torch.sqrt(gamma_expanded)  # Element-wise multiplication\n",
    "#         cov = torch.matmul(z_mu.permute(1, 2, 0), z_mu.permute(1, 0, 2))  # Shape: K x D x D\n",
    "#         cov /= gamma_sum.view(K, 1, 1)\n",
    "\n",
    "#         return phi, mu, cov\n",
    "\n",
    "    # Modified verision that compute only the diagonal elements of the covariance matrices.\n",
    "    def compute_params(self, z, gamma):\n",
    "        \"\"\"Computing the parameters phi, mu and gamma for sample energy function\"\"\"\n",
    "        N, D = z.shape\n",
    "        K = gamma.shape[1]\n",
    "\n",
    "        # Compute phi (mixture weights): Shape K\n",
    "        phi = torch.sum(gamma, dim=0) / N  # Shape: K\n",
    "\n",
    "        # Compute mu (component means): Shape K x D\n",
    "        gamma_sum = torch.sum(gamma, dim=0)  # Shape: K\n",
    "        mu = torch.matmul(gamma.t(), z) / gamma_sum.unsqueeze(1)  # Shape: K x D\n",
    "\n",
    "        # Compute diagonal covariance matrices\n",
    "        z_mu = z.unsqueeze(1) - mu.unsqueeze(0)  # Shape: N x K x D\n",
    "        gamma_expanded = gamma.unsqueeze(2)  # Shape: N x K x 1\n",
    "        z_mu_sq = (z_mu ** 2) * gamma_expanded  # Element-wise multiplication\n",
    "        cov_diag = torch.sum(z_mu_sq, dim=0) / gamma_sum.unsqueeze(1)  # Shape: K x D\n",
    "\n",
    "        return phi, mu, cov_diag\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Original    \n",
    "#     # Computes the energy (negative log-likelihood) of samples under the GMM.\n",
    "#     # If GMM parameters (phi, mu, cov) are not provided, they're computed from z and gamma.\n",
    "#     def compute_energy(self, z, gamma, phi=None, mu=None, cov=None, sample_mean=True):\n",
    "#         \"\"\"Computing the sample energy function\"\"\"\n",
    "#         if (phi is None) or (mu is None) or (cov is None):\n",
    "#             phi, mu, cov = self.compute_params(z, gamma)\n",
    "\n",
    "#         z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "#         eps = 1e-12\n",
    "#         cov_inverse = []\n",
    "#         det_cov = []\n",
    "#         cov_diag = 0\n",
    "#         for k in range(self.n_gmm):\n",
    "#             cov_k = cov[k] + (torch.eye(cov[k].size(-1))*eps).to(self.device)\n",
    "#             cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "#             det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
    "#             cov_diag += torch.sum(1 / cov_k.diag())\n",
    "        \n",
    "#         cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "#         det_cov = torch.cat(det_cov).to(self.device)\n",
    "\n",
    "#         E_z = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "#         E_z = torch.exp(E_z)\n",
    "#         E_z = -torch.log(torch.sum(phi.unsqueeze(0)*E_z / (torch.sqrt(det_cov)).unsqueeze(0), dim=1) + eps)\n",
    "#         if sample_mean==True:\n",
    "#             E_z = torch.mean(E_z)            \n",
    "#         return E_z, cov_diag\n",
    "\n",
    "    \n",
    "#     def compute_params(self, z, gamma):\n",
    "#         \"\"\"Computing the parameters phi, mu and gamma for sample energy function \"\"\" \n",
    "#         # K: number of Gaussian mixture components\n",
    "#         # N: Number of samples\n",
    "#         # D: Latent dimension\n",
    "#         # z = NxD\n",
    "#         # gamma = NxK\n",
    "\n",
    "#         #phi = D\n",
    "#         phi = torch.sum(gamma, dim=0)/gamma.size(0) \n",
    "\n",
    "#         #mu = KxD\n",
    "#         mu = torch.sum(z.unsqueeze(1) * gamma.unsqueeze(-1), dim=0)\n",
    "#         mu /= torch.sum(gamma, dim=0).unsqueeze(-1)\n",
    "\n",
    "#         z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "#         z_mu_z_mu_t = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "        \n",
    "#         #cov = K x D x D\n",
    "#         cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_z_mu_t, dim=0)\n",
    "#         cov /= torch.sum(gamma, dim=0).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "#         return phi, mu, cov\n",
    "    \n",
    "# class Cholesky(torch.autograd.Function):\n",
    "#     def forward(ctx, a):\n",
    "#         l = torch.cholesky(a, False)\n",
    "#         ctx.save_for_backward(l)\n",
    "#         return l\n",
    "#     def backward(ctx, grad_output):\n",
    "#         l, = ctx.saved_variables\n",
    "#         linv = l.inverse()\n",
    "#         inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
    "#             1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
    "#         s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "#         return s\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a9b9022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining DAGMM on HLT_2018\u001b[0m\n",
      "Epoch 0,\tLoss = 25499999999998.53\n",
      "Epoch 1,\tLoss = 25499999999998.53\n",
      "Epoch 2,\tLoss = 25499999999998.53\n",
      "Epoch 3,\tLoss = 25499999999998.53\n",
      "Epoch 4,\tLoss = 25499999999998.53\n",
      "\u001b[1mTraining time:  4875.1405 s\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Training phase\n",
    "num_threads = 16\n",
    "torch.set_num_threads(num_threads)\n",
    "\n",
    "summary_writer = SummaryWriter()\n",
    "\n",
    "testD = testD.to('cpu')\n",
    "testO = testO.to('cpu')\n",
    "\n",
    "if not args.test:\n",
    "    print(f'{color.HEADER}Training {args.model} on {args.dataset}{color.ENDC}')\n",
    "    num_epochs = 5\n",
    "    #num_epochs = 10\n",
    "    #num_epochs = 1\n",
    "    e = epoch + 1\n",
    "    start = time()\n",
    "\n",
    "    for e in list(range(epoch + 1, epoch + num_epochs + 1)):\n",
    "        lossT, lr = backprop(e, model,\n",
    "                                trainD,\n",
    "                                trainO,\n",
    "                                optimizer,\n",
    "                                scheduler,\n",
    "                                True,\n",
    "                                summary_writer,\n",
    "                                args.dataset)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        accuracy_list.append((lossT, lr))\n",
    "\n",
    "    trainD = trainD.cpu()\n",
    "    trainO = trainO.cpu()\n",
    "\n",
    "    print(color.BOLD + 'Training time: ' + \"{:10.4f}\".format(time() - start) + ' s' + color.ENDC)\n",
    "    save_model(model, optimizer, scheduler, e, accuracy_list)\n",
    "    plot_accuracies(accuracy_list, f'{args.model}_{args.dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550388a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5a3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import statistics\n",
    "import os, torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "def smooth(y, box_pts=1):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def plotter(name, y_true, y_pred, ascore, labels):\n",
    "    if 'TranAD' in name: y_true = torch.roll(y_true, 1, 0)\n",
    "    os.makedirs(os.path.join('plots', name), exist_ok=True)\n",
    "    pdf = PdfPages(f'plots/{name}/output.pdf')\n",
    "    for dim in range(y_true.shape[1]):\n",
    "        y_t, y_p, l, a_s = y_true[:, dim], y_pred[:, dim], labels[:, dim], ascore[:, dim]\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title(f'Dimension = {dim}')\n",
    "        # if dim == 0: np.save(f'true{dim}.npy', y_t); np.save(f'pred{dim}.npy', y_p); np.save(f'ascore{dim}.npy', a_s)\n",
    "        ax1.plot(smooth(y_t), linewidth=0.2, label='True')\n",
    "        ax1.plot(smooth(y_p), '-', alpha=0.6, linewidth=0.3, label='Predicted')\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.plot(l, '--', linewidth=0.3, alpha=0.5)\n",
    "        ax3.fill_between(np.arange(l.shape[0]), l, color='blue', alpha=0.3)\n",
    "        if dim == 0: ax1.legend(ncol=2, bbox_to_anchor=(0.6, 1.02))\n",
    "        ax2.plot(smooth(a_s), linewidth=0.2, color='g')\n",
    "        ax2.set_xlabel('Timestamp')\n",
    "        ax2.set_ylabel('Anomaly Score')\n",
    "        pdf.savefig(fig)\n",
    "        plt.close()\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9ae365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spot import SPOT\n",
    "#from src.constants import *\n",
    "from src.prediction_analysis import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Threshold parameters\n",
    "lm_d = {\n",
    "        'SMD': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'machine-1-1': [(0.99995, 1.06), (0.99995, 1.06)],\n",
    "        'machine-2-1': [(0.95, 0.9), (0.95, 0.9)],\n",
    "        'machine-3-2': [(0.99, 1.), (0.99, 1.)],\n",
    "        'machine-3-7': [(0.99995, 1.06), (0.99995, 1.06)],\n",
    "        'synthetic': [(0.999, 1), (0.999, 1)],\n",
    "        'SWaT': [(0.993, 1), (0.993, 1)],\n",
    "        'UCR': [(0.993, 1), (0.99935, 1)],\n",
    "        'NAB': [(0.991, 1), (0.99, 1)],\n",
    "        'SMAP': [(0.98, 1), (0.98, 1)],\n",
    "        'MSL': [(0.97, 1), (0.999, 1.04)],\n",
    "        'WADI': [(0.99, 1), (0.999, 1)],\n",
    "        'MSDS': [(0.91, 1), (0.9, 1.04)],\n",
    "        'MBA': [(0.87, 1), (0.93, 1.04)],\n",
    "        'HLT_DCM_2018': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'HLT_DCM_2022': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'HLT_2018': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'HLT_2023': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'ECLIPSE_MEDIAN': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "        'ECLIPSE_MEAN': [(0.99995, 1.04), (0.99995, 1.06)],\n",
    "    }\n",
    "lm = lm_d[args.dataset][1 if 'TranAD' in args.model else 0]\n",
    "\n",
    "\n",
    "preds = []\n",
    "\n",
    "\n",
    "anomaly_categories = {'Point Global': 0b0000001,\n",
    "                        'Point Contextual': 0b0000010,\n",
    "                        'Persistent Global': 0b0000100,\n",
    "                        'Persistent Contextual': 0b0001000,\n",
    "                        'Collective Global': 0b0010000,\n",
    "                        'Collective Trend': 0b0100000}\n",
    "\n",
    "\n",
    "def calc_point2point(predict, actual):\n",
    "    \"\"\"\n",
    "    calculate f1 score by predict and actual.\n",
    "    Args:\n",
    "        predict (np.ndarray): the predict label\n",
    "        actual (np.ndarray): np.ndarray\n",
    "    \"\"\"\n",
    "    TP = np.sum(predict * actual)\n",
    "    TN = np.sum((1 - predict) * (1 - actual))\n",
    "    FP = np.sum(predict * (1 - actual))\n",
    "    FN = np.sum((1 - predict) * actual)\n",
    "    precision = TP / (TP + FP + 0.00001)\n",
    "    recall = TP / (TP + FN + 0.00001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.00001)\n",
    "    # try:\n",
    "    #     roc_auc = roc_auc_score(actual, predict)\n",
    "    # except:\n",
    "    #     roc_auc = 0\n",
    "    return f1, precision, recall, TP, TN, FP, FN #, roc_auc\n",
    "\n",
    "\n",
    "def adjust_predicts(score, label,\n",
    "                    threshold=None,\n",
    "                    pred=None,\n",
    "                    calc_latency=False):\n",
    "    \"\"\"\n",
    "    Calculate adjusted predict labels using given `score`, `threshold` (or given `pred`) and `label`.\n",
    "    Args:\n",
    "        score (np.ndarray): The anomaly score\n",
    "        label (np.ndarray): The ground-truth label\n",
    "        threshold (float): The threshold of anomaly score.\n",
    "            A point is labeled as \"anomaly\" if its score is lower than the threshold.\n",
    "        pred (np.ndarray or None): if not None, adjust `pred` and ignore `score` and `threshold`,\n",
    "        calc_latency (bool):\n",
    "    Returns:\n",
    "        np.ndarray: predict labels\n",
    "    \"\"\"\n",
    "    if len(score) != len(label):\n",
    "        raise ValueError(\"score and label must have the same length\")\n",
    "    score = np.asarray(score)\n",
    "    label = np.asarray(label)\n",
    "    latency = 0\n",
    "    if pred is None:\n",
    "        predict = score > threshold\n",
    "    else:\n",
    "        predict = pred\n",
    "    actual = label > 0.1\n",
    "    anomaly_state = False\n",
    "    anomaly_count = 0\n",
    "    for i in range(len(score)):\n",
    "        if actual[i] and predict[i] and not anomaly_state:\n",
    "                anomaly_state = True\n",
    "                anomaly_count += 1\n",
    "                for j in range(i, 0, -1):\n",
    "                    if not actual[j]:\n",
    "                        break\n",
    "                    else:\n",
    "                        if not predict[j]:\n",
    "                            predict[j] = True\n",
    "                            latency += 1\n",
    "        elif not actual[i]:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            predict[i] = True\n",
    "    if calc_latency:\n",
    "        return predict, latency / (anomaly_count + 1e-4)\n",
    "    else:\n",
    "        return predict\n",
    "\n",
    "\n",
    "def calc_seq(score, label, threshold, calc_latency=False):\n",
    "    \"\"\"\n",
    "    Calculate f1 score for a score sequence\n",
    "    \"\"\"\n",
    "    if calc_latency:\n",
    "        predict, latency = adjust_predicts(score, label, threshold, calc_latency=calc_latency)\n",
    "        t = list(calc_point2point(predict, label))\n",
    "        t.append(latency)\n",
    "        return t\n",
    "    else:\n",
    "        predict = adjust_predicts(score, label, threshold, calc_latency=calc_latency)\n",
    "        return calc_point2point(predict, label)\n",
    "\n",
    "\n",
    "def bf_search(score, label, start, end=None, step_num=1, display_freq=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
    "    Returns:\n",
    "        list: list for results\n",
    "        float: the `threshold` for best-f1\n",
    "    \"\"\"\n",
    "    if step_num is None or end is None:\n",
    "        end = start\n",
    "        step_num = 1\n",
    "    search_step, search_range, search_lower_bound = step_num, end - start, start\n",
    "    if verbose:\n",
    "        print(\"search range: \", search_lower_bound, search_lower_bound + search_range)\n",
    "    threshold = search_lower_bound\n",
    "    m = (-1., -1., -1.)\n",
    "    m_t = 0.0\n",
    "    for i in range(search_step):\n",
    "        threshold += search_range / float(search_step)\n",
    "        target = calc_seq(score, label, threshold, calc_latency=True)\n",
    "        if target[0] > m[0]:\n",
    "            m_t = threshold\n",
    "            m = target\n",
    "        if verbose and i % display_freq == 0:\n",
    "            print(\"cur thr: \", threshold, target, m, m_t)\n",
    "    print(m, m_t)\n",
    "    return m, m_t\n",
    "\n",
    "\n",
    "def pot_eval(init_score, score, label, q=1e-3, level=0.02):\n",
    "    \"\"\"\n",
    "    Run POT method on given score.\n",
    "    Args:\n",
    "        init_score (np.ndarray): The data to get init threshold.\n",
    "            it should be the anomaly score of train set.\n",
    "        score (np.ndarray): The data to run POT method.\n",
    "            it should be the anomaly score of test set.\n",
    "        label:\n",
    "        q (float): Detection level (risk)\n",
    "        level (float): Probability associated with the initial threshold t\n",
    "    Returns:\n",
    "        dict: pot result dict\n",
    "    \"\"\"\n",
    "    lms = lm[0]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            s = SPOT(q)  # SPOT object\n",
    "            s.fit(init_score, score)  # data import\n",
    "            s.initialize(level=lms, min_extrema=False, verbose=False)  # initialization step\n",
    "        except: lms = lms * 0.999\n",
    "        else: break\n",
    "    ret = s.run(dynamic=False)  # run\n",
    "    # print(len(ret['alarms']))\n",
    "    # print(len(ret['thresholds']))\n",
    "    pot_th = np.mean(ret['thresholds']) * lm[1]\n",
    "    # pot_th = np.percentile(score, 100 * lm[0])\n",
    "    # np.percentile(score, 100 * lm[0])\n",
    "\n",
    "    label = label[:len(score)]\n",
    "\n",
    "    pred = np.zeros_like(score, dtype=np.uint8)\n",
    "\n",
    "    pred[ret['alarms']] = 1\n",
    "\n",
    "    pred = adjust_predicts(pred, label, 0.1)\n",
    "\n",
    "    mcc = matthews_corrcoef(label, pred)\n",
    "\n",
    "#     mean_shift, var_shift = get_mean_and_var_shift(score, label)\n",
    "# \n",
    "#     prob_pred = estimate_prediction_probability(score > pot_th,  label > 0.1)\n",
    "\n",
    "    latencies, mean_latency, var_latency =\\\n",
    "                get_detection_latencies(score > pot_th,  label > 0.1)\n",
    "\n",
    "\n",
    "    # pred, p_latency = adjust_predicts(score, label, pot_th, calc_latency=True)\n",
    "\n",
    "    p_t = calc_point2point(pred, label)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(label, score)\n",
    "    except:\n",
    "         auroc = 0\n",
    "\n",
    "\n",
    "    # print('POT result: ', p_t, pot_th, p_latency)\n",
    "\n",
    "    return {\n",
    "        'f1': p_t[0],\n",
    "        'precision': p_t[1],\n",
    "        'recall': p_t[2],\n",
    "        'TP': p_t[3],\n",
    "        'TN': p_t[4],\n",
    "        'FP': p_t[5],\n",
    "        'FN': p_t[6],\n",
    "        'ROC/AUC': auroc,\n",
    "        'MCC': mcc,\n",
    "        'threshold': pot_th,\n",
    "        'mean latency': mean_latency,\n",
    "        'latency var': var_latency,\n",
    "    }, np.array(pred), latencies\n",
    "\n",
    "\n",
    "def save_metrics_to_csv(filename,\n",
    "                            row_name,\n",
    "                            pred_train,\n",
    "                            pred_test,\n",
    "                            true,\n",
    "                            q=1e-5,\n",
    "                            level=0.02):\n",
    "\n",
    "    true = true[:len(pred_test)]\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(true, pred_test)\n",
    "    except:\n",
    "        auroc = 0\n",
    "\n",
    "    lms = lm[0]\n",
    "    while True:\n",
    "        try:\n",
    "            s = SPOT(q)  # SPOT object\n",
    "            s.fit(pred_train, pred_test)  # data import\n",
    "            s.initialize(level=lms, min_extrema=False, verbose=False)  # initialization step\n",
    "        except: lms = lms * 0.999\n",
    "        else: break\n",
    "    ret = s.run(dynamic=False)  # run\n",
    "    # print(len(ret['alarms']))\n",
    "    # print(len(ret['thresholds']))\n",
    "    pot_th = np.mean(ret['thresholds']) * lm[1]\n",
    "    # pot_th = np.percentile(score, 100 * lm[0])\n",
    "    # np.percentile(score, 100 * lm[0])\n",
    "\n",
    "    mean_shift, var_shift = get_mean_and_var_shift(pred_test, true)\n",
    "\n",
    "    prob_pred = estimate_prediction_probability(pred_test > pot_th,  true > 0.1)\n",
    "\n",
    "    latencies, mean_latency, var_latency =\\\n",
    "                get_detection_latencies(pred_test > pot_th,  true > 0.1)\n",
    "\n",
    "    pred, p_latency = adjust_predicts(pred_test, true, pot_th, calc_latency=True)\n",
    "\n",
    "    # DEBUG - np.save(f'{debug}.npy', np.array(pred))\n",
    "    # DEBUG - print(np.argwhere(np.array(pred)))\n",
    "\n",
    "    p_t = calc_point2point(pred, true)\n",
    "\n",
    "    # print('POT result: ', p_t, pot_th, p_latency)\n",
    "\n",
    "    mcc = matthews_corrcoef(true, pred)\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            header = ['Name',\n",
    "                        'AUROC',\n",
    "                        'F1',\n",
    "                        'MCC',\n",
    "                        'Precision',\n",
    "                        'Recall'\n",
    "                        'Threshold']\n",
    "\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "    with open(filename, 'a') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow([row_name,\n",
    "                                auroc,\n",
    "                                p_t[0],\n",
    "                                mcc,\n",
    "                                p_t[1],\n",
    "                                p_t[2],\n",
    "                                pot_th])\n",
    "\n",
    "\n",
    "def print_metric_comparison_by_categories(pred_train,\n",
    "                                                pred_test,\n",
    "                                                true,\n",
    "                                                categories,\n",
    "                                                q=1e-5,\n",
    "                                                level=0.02):\n",
    "\n",
    "    true = true[:len(pred_test)]\n",
    "    categories = categories[:len(pred_test)]\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(true, pred_test)\n",
    "    except:\n",
    "        auroc = 0\n",
    "\n",
    "    lms = lm[0]\n",
    "    while True:\n",
    "        try:\n",
    "            s = SPOT(q)  # SPOT object\n",
    "            s.fit(pred_train, pred_test)  # data import\n",
    "            s.initialize(level=lms, min_extrema=False, verbose=False)  # initialization step\n",
    "        except: lms = lms * 0.999\n",
    "        else: break\n",
    "    ret = s.run(dynamic=False)  # run\n",
    "    # print(len(ret['alarms']))\n",
    "    # print(len(ret['thresholds']))\n",
    "    pot_th = np.mean(ret['thresholds']) * lm[1]\n",
    "    # pot_th = np.percentile(score, 100 * lm[0])\n",
    "    # np.percentile(score, 100 * lm[0])\n",
    "\n",
    "    pred_adjusted_transformer =\\\n",
    "                adjust_predicts(pred_test,\n",
    "                                    true,\n",
    "                                    pot_th)\n",
    "\n",
    "    print('\\nBy Category:\\n')\n",
    "\n",
    "    for category, flag in anomaly_categories.items():\n",
    "        print(category)\n",
    "\n",
    "        mask = np.where(categories & flag, 1, 0)\n",
    "\n",
    "        mask = np.logical_or(mask,\n",
    "                    np.where(categories == 0, 1, 0))\n",
    "\n",
    "        f1 = f1_score(true[mask],\n",
    "                        pred_adjusted_transformer[mask])\n",
    "\n",
    "        precision = precision_score(true[mask],\n",
    "                        pred_adjusted_transformer[mask])\n",
    "\n",
    "        recall = recall_score(true[mask],\n",
    "                        pred_adjusted_transformer[mask])\n",
    "\n",
    "        mcc = matthews_corrcoef(true[mask],\n",
    "                    pred_adjusted_transformer[mask])\n",
    "\n",
    "        p = estimate_prediction_probability(\n",
    "                                pred_test[mask] > pot_th,\n",
    "                                true[mask])\n",
    "\n",
    "        print(f'\\n\\tPrecision: {precision:.3f}'\n",
    "                f' Recall: {recall:.3f}'\n",
    "                f' F1 score: {f1:.3f}'\n",
    "                f' MCC: {mcc:.3f}'\n",
    "                f' p: {p:.5f}')\n",
    "                \n",
    "\n",
    "    f1 = f1_score(true, pred_adjusted_transformer)\n",
    "\n",
    "    precision = precision_score(true,\n",
    "                    pred_adjusted_transformer)\n",
    "\n",
    "    recall = recall_score(true,\n",
    "                    pred_adjusted_transformer)\n",
    "\n",
    "    mcc = matthews_corrcoef(true,\n",
    "                pred_adjusted_transformer)\n",
    "\n",
    "    p = estimate_prediction_probability(\n",
    "                            pred_test > pot_th,\n",
    "                            true)\n",
    "        \n",
    "\n",
    "    print('\\nAll categories:\\n')\n",
    "\n",
    "    print(f'2nd Stage w/o Clustering'\n",
    "            f'\\n\\tPrecision: {precision:.3f}'\n",
    "            f' Recall: {recall:.3f}'\n",
    "            f' F1 score: {f1:.3f}'\n",
    "            f' MCC: {mcc:.3f}'\n",
    "            f' for threshold {pot_th:.10f}'\n",
    "            f'\\n\\nAUROC: {auroc:.3f}'\n",
    "            f' p: {p:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578caad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args_dict = {\n",
    "    \"use_gpu\": False,\n",
    "    \"use_multi_gpu\": False,\n",
    "    \"seed\":42, \n",
    "    \"dataset\": \"HLT_2018\",\n",
    "    #\"dataset\": \"HLT_2023\",\n",
    "    #\"dataset\": \"HLT_DCM_2023\",\n",
    "    \"checkpoints\": \"./checkpoints\",\n",
    "    \"loss\": \"MSE\",\n",
    "    \"learning_rate\": 0.00009727998365755187,\n",
    "    \"num_workers\": 8, # Before was 0\n",
    "    \"train_epochs\": 10,\n",
    "    \"batch_size\": 256, # Before was 128\n",
    "    \"patience\": 3,\n",
    "    \"model\": \"DAGMM\",\n",
    "    #\"retrain\": True,\n",
    "    \"retrain\": False,\n",
    "    #\"test\": False,\n",
    "    \"test\": True,\n",
    "    #\"apply_augmentations\": True,\n",
    "    #\"apply_augmentations\": False,\n",
    "    #\"augmentations\": ['Scale:0.8,1.0', 'Scale_APP:0.8,1.0,0.01,0.05,0.05'],\n",
    "    #\"augmentations\": ['Scale:1.0,1.2', 'Scale_APP:1.0,1.2,0.01,0.01,0.01'], \n",
    "    #\"augmentations\": [],\n",
    "    #\"augmented_dataset_size_relative\": 1.0,\n",
    "    #\"augmented_data_ratio\": 0.25\n",
    "    #\"augmented_data_ratio\": 0\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an argparse.Namespace object\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48e6e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTesting DAGMM on HLT_2018\u001b[0m\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_456/2771643375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtestO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_456/3084737140.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(epoch, model, data, dataO, optimizer, scheduler, training, summary_writer, dataset_name)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mz_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0msample_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0manomaly_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_energy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0manomaly_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_456/2837749885.py\u001b[0m in \u001b[0;36mcompute_energy\u001b[0;34m(self, z, gamma, phi, mu, cov_diag, sample_mean)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m\"\"\"Computing the sample energy function with diagonal covariance matrices\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcov_diag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_456/2837749885.py\u001b[0m in \u001b[0;36mcompute_params\u001b[0;34m(self, z, gamma)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;34m\"\"\"Computing the parameters phi, mu and gamma for sample energy function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Compute phi (mixture weights): Shape K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "### Testing phase\n",
    "\n",
    "torch.zero_grad = True\n",
    "model.eval()\n",
    "print(f'{color.HEADER}Testing {args.model} on {args.dataset}{color.ENDC}')\n",
    "\n",
    "testD = testD.to(device)\n",
    "testO = testO.to(device)\n",
    "\n",
    "loss, y_pred = backprop(0, model, testD, testO, optimizer, scheduler, training=False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "testD = testD.cpu()\n",
    "testO = testO.cpu()\n",
    "# labels = labels.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d36285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot curves\n",
    "\n",
    "if not args.test:\n",
    "    if 'TranAD' in model.name:\n",
    "        testO = torch.roll(testO, 1, 0)\n",
    "\n",
    "    plotter(f'{args.model}_{args.dataset}', testO, y_pred, loss, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73f5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227abc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scores\n",
    "\n",
    "testD = testD.to(device)\n",
    "testO = testO.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "lossT, _ = backprop(0, model, trainD.to(device), trainO.to(device), optimizer, scheduler, training=False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(loss.shape[1]):\n",
    "    lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]\n",
    "    result, pred, _ = pot_eval(lt, l, ls); preds.append(pred)\n",
    "    df = df.append(result, ignore_index=True)\n",
    "\n",
    "lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "labelsFinal = (np.sum(labels, axis=1) >= 1) + 0\n",
    "result, _, latencies = pot_eval(lossTfinal, lossFinal, labelsFinal)\n",
    "\n",
    "if 'HLT' in args.dataset:\n",
    "\n",
    "    variant = f'{args.dataset.split(\"_\")[-2].lower()}_'\\\n",
    "                            f'{args.dataset.split(\"_\")[-1]}'\n",
    "\n",
    "    augment_label = 'no_augment_' if augmentation_string == 'no_augment' else ''\n",
    "\n",
    "    _save_numpy_array(lossTfinal,\n",
    "                        f'evaluation/reduced_detection_{variant}/'\\\n",
    "                                        f'predictions/{args.model.lower()}_'\\\n",
    "                                        f'train_{augment_label}seed_{int(args.seed)}.npy')\n",
    "\n",
    "    _save_numpy_array(lossTfinal,\n",
    "                        f'evaluation/combined_detection_{variant}/'\\\n",
    "                                        f'predictions/{args.model.lower()}_'\\\n",
    "                                        f'train_{augment_label}seed_{int(args.seed)}.npy')\n",
    "\n",
    "    _save_numpy_array(lossFinal,\n",
    "                        f'evaluation/reduced_detection_{variant}/'\\\n",
    "                                        f'predictions/{args.model.lower()}_'\\\n",
    "                                        f'{augment_label}seed_{int(args.seed)}.npy')\n",
    "\n",
    "else:\n",
    "    metrics_to_save = [int(args.seed),\n",
    "                            result['ROC/AUC'],\n",
    "                            result['f1'],\n",
    "                            result['MCC'],\n",
    "                            result['precision'],\n",
    "                            result['recall']]\n",
    "\n",
    "    metrics_to_save = np.atleast_2d(metrics_to_save)\n",
    "\n",
    "    metrics_to_save_pd = pd.DataFrame(data=metrics_to_save)\n",
    "    metrics_to_save_pd.to_csv(f'results_{args.model.lower()}_{args.dataset}.csv',\n",
    "                                                                        mode='a+',\n",
    "                                                                        header=False,\n",
    "                                                                        index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a15ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a397fd99",
   "metadata": {},
   "source": [
    "# Plotting the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327adedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_array(filename: str):\n",
    "    with open(filename, 'rb') as output_file:\n",
    "        return np.load(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77566ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholded(pred_train, pred_test, true, q=1e-3, level=0.8):\n",
    "    \"\"\"\n",
    "    Run POT method on given score.\n",
    "    Args:\n",
    "        init_score (np.ndarray): The data to get init threshold.\n",
    "            For `OmniAnomaly`, it should be the anomaly score of train set.\n",
    "        score (np.ndarray): The data to run POT method.\n",
    "            For `OmniAnomaly`, it should be the anomaly score of test set.\n",
    "        label:\n",
    "        q (float): Detection level (risk)\n",
    "        level (float): Probability associated with the initial threshold t\n",
    "    Returns:\n",
    "        dict: pot result dict\n",
    "    \"\"\"\n",
    "\n",
    "    # SPOT object\n",
    "    spot = SPOT(q)\n",
    "\n",
    "    # data import\n",
    "    spot.fit(pred_train, pred_test)\n",
    "\n",
    "    # initialization step\n",
    "    spot.initialize(level=level,\n",
    "                    min_extrema=False,\n",
    "                    verbose=False)\n",
    "\n",
    "    # run\n",
    "    ret = spot.run()\n",
    "\n",
    "    pred = np.zeros_like(pred_test, dtype=np.uint8)\n",
    "\n",
    "    pred[ret['alarms']] = 1\n",
    "\n",
    "    pred = adjust_predicts(pred, true, 0.1)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_thresholded_tranad(pred_train, pred_test, true, q=1e-3, level=0.02):\n",
    "    \"\"\"\n",
    "    Run POT method on given score.\n",
    "    Args:\n",
    "        init_score (np.ndarray): The data to get init threshold.\n",
    "            it should be the anomaly score of train set.\n",
    "        score (np.ndarray): The data to run POT method.\n",
    "            it should be the anomaly score of test set.\n",
    "        label:\n",
    "        q (float): Detection level (risk)\n",
    "        level (float): Probability associated with the initial threshold t\n",
    "    Returns:\n",
    "        dict: pot result dict\n",
    "    \"\"\"\n",
    "\n",
    "    lms = 0.99995\n",
    "    while True:\n",
    "        try:\n",
    "            s = SPOT(q)  # SPOT object\n",
    "            s.fit(pred_train, pred_test)  # data import\n",
    "            s.initialize(level=lms, min_extrema=False, verbose=False)  # initialization step\n",
    "        except: lms = lms * 0.999\n",
    "        else: break\n",
    "    ret = s.run(dynamic=False)  # run\n",
    "    # print(len(ret['alarms']))\n",
    "    # print(len(ret['thresholds']))\n",
    "    pot_th = np.mean(ret['thresholds']) * 0.6\n",
    "    # pot_th = np.percentile(score, 100 * lm[0])\n",
    "    # np.percentile(score, 100 * lm[0])\n",
    "\n",
    "    pred = pred_test > pot_th\n",
    "\n",
    "    pred = adjust_predicts(pred, true, 0.1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalous_runs(x):\n",
    "    '''\n",
    "    Find runs of consecutive items in an array.\n",
    "    As published in https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065\n",
    "    '''\n",
    "\n",
    "    # Ensure array\n",
    "\n",
    "    x = np.asanyarray(x)\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError('Only 1D arrays supported')\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Handle empty array\n",
    "\n",
    "    if n == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Find run starts\n",
    "\n",
    "        loc_run_start = np.empty(n, dtype=bool)\n",
    "        loc_run_start[0] = True\n",
    "\n",
    "        np.not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n",
    "        run_starts = np.nonzero(loc_run_start)[0]\n",
    "\n",
    "        # Find run values\n",
    "        run_values = x[loc_run_start]\n",
    "\n",
    "        # Find run lengths\n",
    "        run_lengths = np.diff(np.append(run_starts, n))\n",
    "\n",
    "        run_starts = np.compress(run_values, run_starts)\n",
    "        run_lengths = np.compress(run_values, run_lengths)\n",
    "\n",
    "        run_ends = run_starts + run_lengths\n",
    "\n",
    "        return run_starts, run_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data: np.array,\n",
    "                    label: np.array,\n",
    "                    timestamps,\n",
    "                    tranad_seed: int):\n",
    "\n",
    "    label = np.any(label, axis=1).astype(np.uint8)\n",
    "\n",
    "    #preds_clustering = load_numpy_array('predictions/clustering.npy')\n",
    "    #preds_tranad = load_numpy_array(f'predictions/tranad_seed_{tranad_seed}.npy')\n",
    "    \n",
    "    #preds_tranad = load_numpy_array(f'/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/notebooks/evaluation/reduced_detection_hlt_2018/predictions/dagmm_no_augment_seed_42.npy')\n",
    "    #preds_tranad_train = load_numpy_array(f'/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/notebooks/evaluation/reduced_detection_hlt_2018/predictions/dagmm_train_no_augment_seed_42.npy')\n",
    "     \n",
    "    preds_tranad = load_numpy_array(f'/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/notebooks/evaluation/reduced_detection_hlt_2018/predictions/dagmm_seed_42.npy')\n",
    "    preds_tranad_train = load_numpy_array(f'/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/notebooks/evaluation/reduced_detection_hlt_2018/predictions/dagmm_train_seed_42.npy')        \n",
    "        \n",
    "\n",
    "    \n",
    "    #preds_tranad_train = load_numpy_array(f'predictions/tranad_train_seed_{tranad_seed}.npy')\n",
    "    #preds_l2_dist_train_mse = load_numpy_array(f'predictions/l2_dist_train_mse_seed_{informer_mse_seed}.npy')\n",
    "    #preds_l2_dist_mse = load_numpy_array(f'predictions/l2_dist_mse_seed_{informer_mse_seed}.npy')\n",
    "    #preds_l2_dist_train_smse = load_numpy_array(f'predictions/l2_dist_train_smse_seed_{informer_smse_seed}.npy')\n",
    "    #preds_l2_dist_smse = load_numpy_array(f'predictions/l2_dist_smse_seed_{informer_smse_seed}.npy')\n",
    "\n",
    "    #spot_train_size = int(len(preds_l2_dist_mse)*0.1)\n",
    "\n",
    "    # Fix alignment\n",
    "\n",
    "    pbar = tqdm(total=4, desc='Preprocessing')\n",
    "\n",
    "    #     preds_l2_dist_mse =\\\n",
    "    #         np.pad(preds_l2_dist_mse[1:], (0, 1),\n",
    "    #                                 'constant',\n",
    "    #                                 constant_values=(0,))\n",
    "\n",
    "    #     preds_l2_dist_smse =\\\n",
    "    #         np.pad(preds_l2_dist_smse[1:], (0, 1),\n",
    "    #                                 'constant',\n",
    "    #                                 constant_values=(0,))\n",
    "    \n",
    "    # label = label[:16000]\n",
    "    # preds_clustering = preds_clustering[:16000, :]\n",
    "    # preds_tranad = preds_tranad[:16000, :]\n",
    "    # preds_l2_dist_mse = preds_l2_dist_mse[:16000, :]\n",
    "    # preds_l2_dist_smse = preds_l2_dist_smse[:16000, :]\n",
    "\n",
    "    #preds_clustering =\\\n",
    "    #    adjust_predicts(preds_clustering,\n",
    "    #                            label, 0.1)\n",
    "    \n",
    "    #pbar.update(1)\n",
    "\n",
    "    preds_tranad =\\\n",
    "        get_thresholded_tranad(preds_tranad_train,\n",
    "                            preds_tranad, label, 0.01)\n",
    "\n",
    "    #preds_strada_tranad = np.logical_or(preds_clustering,\n",
    "    #                                            preds_tranad)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "    #preds_l2_dist_mse =\\\n",
    "    #    get_thresholded(preds_l2_dist_train_mse[:spot_train_size],\n",
    "    #                                            preds_l2_dist_mse, label, 0.0025)\n",
    "\n",
    "    #preds_strada_mse = np.logical_or(preds_clustering,\n",
    "    #                                    preds_l2_dist_mse)\n",
    "\n",
    "    #pbar.update(1)\n",
    "\n",
    "    #preds_l2_dist_smse =\\\n",
    "    #    get_thresholded(preds_l2_dist_train_smse[:spot_train_size],\n",
    "    #                                            preds_l2_dist_smse, label, 0.008)\n",
    "    \n",
    "    #preds_strada_smse = np.logical_or(preds_clustering,\n",
    "    #                                    preds_l2_dist_smse)\n",
    "\n",
    "    #pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    #     preds_all = {   'T-DBSCAN': preds_clustering,\n",
    "    #                     'Informer-MSE': preds_l2_dist_mse,\n",
    "    #                     'Informer-SMSE': preds_l2_dist_smse,\n",
    "    #                     'TranAD': preds_tranad,\n",
    "    #                     'STRADA-MSE': preds_strada_mse,\n",
    "    #                     'STRADA-SMSE': preds_strada_smse,\n",
    "    #                     'STRADA-TranAD': preds_strada_tranad}\n",
    "    \n",
    "    preds_all = {  'TranAD': preds_tranad }\n",
    "    \n",
    "    #     preds_all = {   'T-DBSCAN': preds_clustering,\n",
    "    #                 'Informer-MSE': preds_l2_dist_mse,\n",
    "    #                 'Informer-SMSE': preds_l2_dist_smse,\n",
    "    #                 'TranAD': preds_tranad,\n",
    "    #                 'STRADA-MSE': preds_strada_mse,\n",
    "    #                 'STRADA-SMSE': preds_strada_smse,\n",
    "    #                 'STRADA-TranAD': preds_strada_tranad}\n",
    "\n",
    "    # These colors are specifically chosen to improve\n",
    "    # accessibility for readers with colorblindness\n",
    "\n",
    "    colors = {  'TranAD': '#D4FC14' }\n",
    "\n",
    "    positions = { 'TranAD': 0 }\n",
    "    \n",
    "    SMALL_SIZE = 13\n",
    "    MEDIUM_SIZE = 13\n",
    "    BIGGER_SIZE = 13\n",
    "\n",
    "    # xlims = [(0, 17000),\n",
    "    #             (200, 2000),\n",
    "    #             (10000, 13500)]\n",
    "    \n",
    "    xlims = [(0, len(preds_tranad)),]\n",
    "    \n",
    "    plt.rc('font', size=SMALL_SIZE)\n",
    "    plt.rc('axes', titlesize=BIGGER_SIZE)\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)\n",
    "\n",
    "    for index, (xlim_lower, xlim_upper) in enumerate(tqdm(xlims,\n",
    "                                                    desc='Plotting')):\n",
    "\n",
    "        fig, (ax_data, ax_pred) = plt.subplots(2, 1, figsize=(16, 9), dpi=300)\n",
    "\n",
    "        plt.yticks(rotation=30, ha='right')\n",
    "\n",
    "        ax_data.set_title('Data')\n",
    "        ax_data.set_xlabel('Timestep')\n",
    "\n",
    "        ax_data.grid()\n",
    "\n",
    "        ax_data.set_xticks(np.arange(len(timestamps)),\n",
    "                                            timestamps,\n",
    "                                            rotation=30,\n",
    "                                            ha='right')\n",
    "        \n",
    "        ax_data.xaxis.set_major_locator(plt.MaxNLocator(8))\n",
    "\n",
    "        ax_data.set_xlim(xlim_lower,\n",
    "                            xlim_upper)\n",
    "        \n",
    "        ax_data.set_ylim(-1, 100)\n",
    "\n",
    "        ax_data.plot(data,\n",
    "                        linewidth=0.9,\n",
    "                        color='k')\n",
    "\n",
    "        anomaly_starts, anomaly_ends =\\\n",
    "                    get_anomalous_runs(label)\n",
    "\n",
    "        for start, end in zip(anomaly_starts,\n",
    "                                    anomaly_ends):\n",
    "            ax_data.axvspan(start, end, color='red', alpha=0.5)\n",
    "            ax_pred.axvspan(start, end, color='red', alpha=0.5)\n",
    "\n",
    "        ax_pred.set_xticks(np.arange(len(timestamps)),\n",
    "                                            timestamps,\n",
    "                                            rotation=30,\n",
    "                                            ha='right')\n",
    "        \n",
    "        ax_pred.xaxis.set_major_locator(plt.MaxNLocator(8))\n",
    "\n",
    "        ax_pred.set_yticks(list(positions.values()),\n",
    "                                list(positions.keys()))\n",
    "\n",
    "        ax_pred.set_title('Predictions')\n",
    "        ax_pred.set_xlabel('Timestep')\n",
    "        ax_pred.set_ylabel('Method')\n",
    "\n",
    "        ax_pred.set_xlim(xlim_lower,\n",
    "                            xlim_upper)\n",
    "\n",
    "        for method, preds in preds_all.items():\n",
    "            pred_starts, pred_ends =\\\n",
    "                get_anomalous_runs(preds)\n",
    "                \n",
    "            for start, end in zip(pred_starts, pred_ends):\n",
    "\n",
    "                length = end - start\n",
    "\n",
    "                ax_pred.barh(positions[method],\n",
    "                                length,\n",
    "                                left=start,\n",
    "                                color=colors[method],\n",
    "                                edgecolor='k',\n",
    "                                linewidth=0.7,\n",
    "                                label=method,\n",
    "                                height=0.85)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/prediction_comparison_{index}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb82884",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlt_data_pd = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/unreduced_hlt_test_set_2018_x.h5')\n",
    "print(hlt_data_pd.shape)\n",
    "hlt_data_pd.fillna(0, inplace=True)\n",
    "\n",
    "hlt_data_np = hlt_data_pd.to_numpy()\n",
    "\n",
    "labels_pd = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/unreduced_hlt_test_set_2018_y.h5')\n",
    "print(labels_pd.shape)\n",
    "labels_np = labels_pd.to_numpy()\n",
    "\n",
    "timestamps = labels_pd.index\n",
    "\n",
    "\n",
    "# Convert the index to datetime format\n",
    "timestamps_datetime = pd.to_datetime(timestamps)\n",
    "\n",
    "# Apply strftime to format the datetime objects\n",
    "formatted_timestamps = timestamps_datetime.strftime('%d.%m %H:%M')\n",
    "\n",
    "labels_np = np.greater_equal(labels_np, 1)\n",
    "\n",
    "plot_results(hlt_data_np,\n",
    "                labels_np,\n",
    "                formatted_timestamps,\n",
    "                42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a025c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4471634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9f531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad4b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60c3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc2154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a27a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439379f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
