{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03ddc7b",
   "metadata": {},
   "source": [
    "# Training the Informer with 2023 data\n",
    "\n",
    "Following the \"informer.py\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d7eac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#!pip install bigtree\n",
    "import site\n",
    "site.getusersitepackages()\n",
    "\n",
    "sys.path.append(site.getusersitepackages())\n",
    "#export PYTHONPATH=$PYTHONPATH:/eos/home-i01/j/jhoya/.local/lib/python3.9/site-packages\n",
    "sys.path.append('/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/transformer_based_detection/informers/')\n",
    "from exp.exp_informer import ExpInformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87b9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args_dict = {\n",
    "    \"use_gpu\": False,\n",
    "    \"use_multi_gpu\": False,\n",
    "    \"seed\":42, \n",
    "    \"data\": \"HLT_DCM_2023\",\n",
    "    \"features\": \"M\",\n",
    "    \"target\": \"OT\",\n",
    "    \"freq\": \"s\",\n",
    "    \"checkpoints\": \"./checkpoints\",\n",
    "    \"seq_len\": 16,\n",
    "    \"label_len\": 8,\n",
    "    \"pred_len\": 1,\n",
    "    \"enc_in\": 146,\n",
    "    \"dec_in\": 146,\n",
    "    \"c_out\": 146,\n",
    "    \"d_model\": 256, # before was 576\n",
    "    \"n_heads\": 2, # before was 4\n",
    "    \"e_layers\": 1,\n",
    "    \"d_layers\": 4,\n",
    "    \"d_ff\": 1024, # before was 2944\n",
    "    \"factor\": 1,\n",
    "    \"padding\": 1,\n",
    "    \"dropout\": 0.001,\n",
    "    \"attn\": \"prob\",\n",
    "    \"embed\": \"timeF\",\n",
    "    \"activation\": \"gelu\",\n",
    "    \"output_attention\": True,\n",
    "    \"no_distil\": True,\n",
    "    \"no_mix\": True,\n",
    "    \"lradj\": \"type1\",\n",
    "    \"use_amp\": False,\n",
    "    \"inverse\": False,\n",
    "    \"loss\": \"MSE\",\n",
    "    \"learning_rate\": 0.00009727998365755187,\n",
    "    \"num_workers\": 8, # Before was 0\n",
    "    \"train_epochs\": 2,\n",
    "    \"batch_size\": 256, # Before was 128\n",
    "    \"patience\": 3,\n",
    "    \"apply_augmentations\": False,\n",
    "    #\"augmentations\": ['Scale:0.8,1.0', 'Scale_APP:0.8,1.0,0.01,0.05,0.05'],\n",
    "    \"augmentations\": [],\n",
    "    \"augmented_dataset_size_relative\": 1.0,\n",
    "    \"augmented_data_ratio\": 0.25\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an argparse.Namespace object\n",
    "args = argparse.Namespace(**args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ca0e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = ExpInformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e89d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hlt_dcm_2023_mse_seed_42'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting = f'{args.data.lower()}_{args.loss.lower()}_seed_{int(args.seed)}'\n",
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14c1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "# Set experiments\n",
    "exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85be42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.train(setting)\n",
    "#exp.test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d652724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import partial, partialmethod\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from fvcore.nn import FlopCountAnalysis, ActivationCountAnalysis\n",
    "#from torchinfo import summary\n",
    "from bigtree import Node, tree_to_dataframe, tree_to_dot\n",
    "from bigtree.tree.search import find_child_by_name\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset_loaders.omni_anomaly_dataset import OmniAnomalyDataset\n",
    "from dataset_loaders.hlt_datasets import HLTDataset\n",
    "from dataset_loaders.eclipse_datasets import EclipseDataset\n",
    "from exp.exp_basic import ExpBasic\n",
    "from models.model import Informer\n",
    "from models.sad_like_loss import *\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "693263e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "train_data, train_loader = exp._get_data(flag='train')\n",
    "vali_data, vali_loader = exp._get_data(flag='val')\n",
    "path = os.path.join(exp.args.checkpoints, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f3aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/hlt_dcm_2023_mse_seed_42'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "train_data.pickle_scaler(f'{path}/scaler.pkl')\n",
    "if exp.args.loss == 'SMSE':\n",
    "    labeled_train_data, labeled_train_loader =\\\n",
    "                    exp._get_data(flag='labeled_train')\n",
    "train_steps_unlabeled = len(train_loader)\n",
    "train_steps_labeled = len(labeled_train_loader)\\\n",
    "            if exp.args.loss == 'SMSE' else 0\n",
    "delta = -1 if exp.args.loss == 'SMSE' else 0\n",
    "early_stopping = EarlyStopping(patience=exp.args.patience,\n",
    "                                                verbose=True,\n",
    "                                                delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04dcaa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 28\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "n_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {n_cores}\")\n",
    "# Set the number of threads for PyTorch\n",
    "import torch\n",
    "#n_threads = os.cpu_count()  # Use all available CPU cores\n",
    "#n_threads = max(1, os.cpu_count() // 2)\n",
    "n_threads = 16\n",
    "print(n_threads)\n",
    "torch.set_num_threads(n_threads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14d3e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_optim = exp._select_optimizer()\n",
    "criterion = exp._select_criterion(exp.args.loss)\n",
    "if exp.args.use_amp:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "summary_writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# Switch to Lower Precision: Use torch.float16 instead of torch.float32 \n",
    "exp.model = exp.model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933a05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9364f89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(exp.args.train_epochs):\n",
    "    train_loss = []\n",
    "    preds_all = []\n",
    "    y_actual_all =[]\n",
    "        \n",
    "    exp.model.train() #set the model to training mode\n",
    "        \n",
    "    epoch_time = time.time()\n",
    "    #print(epoch_time)\n",
    "        \n",
    "\n",
    "\n",
    "    for batch_index, (batch_x,\\\n",
    "                        batch_y,\\\n",
    "                        batch_x_mark,\\\n",
    "                        batch_y_mark) in enumerate(tqdm(train_loader)):\n",
    "                \n",
    "        model_optim.zero_grad()\n",
    "        if exp.args.output_attention:\n",
    "            #print(train_data)\n",
    "            #print(batch_x)\n",
    "            #print(batch_y)\n",
    "            #print(batch_x_mark)\n",
    "            #print(batch_y_mark)\n",
    "            pred, true, _ = exp._process_one_batch(train_data,\n",
    "                                                        batch_x,\n",
    "                                                        batch_y,\n",
    "                                                        batch_x_mark,\n",
    "                                                        batch_y_mark)\n",
    "        else:\n",
    "            pred, true = exp._process_one_batch(train_data,\n",
    "                                                    batch_x,\n",
    "                                                    batch_y,\n",
    "                                                    batch_x_mark,\n",
    "                                                    batch_y_mark)\n",
    "        #print(pred)\n",
    "        preds_all.append(pred.detach().cpu().numpy())\n",
    "        y_actual_all.append(true.detach().cpu().numpy())\n",
    "        loss = criterion(pred, true)\n",
    "        train_loss.append(loss.item())\n",
    "        #print(preds_all)\n",
    "        #print(y_actual_all)\n",
    "        #print(train_loss)\n",
    "        \n",
    "        summary_writer.add_scalar(\"Train loss\",\n",
    "                                    loss,\n",
    "                                    batch_index + epoch*\\\n",
    "                                        train_steps_unlabeled)\n",
    "                 \n",
    "        if exp.args.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(model_optim)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "            \n",
    "    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))    \n",
    "    train_loss = np.average(train_loss)\n",
    "    vali_loss = exp.vali(vali_data,\n",
    "                            vali_loader,\n",
    "                            criterion)\n",
    "    preds_all = early_stopping(vali_loss,\n",
    "                                exp.model,\n",
    "                                preds_all,\n",
    "                                path)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    adjust_learning_rate(model_optim, epoch + 1, exp.args)\n",
    "    summary_writer.add_scalar(\"Validation loss\", vali_loss, epoch)\n",
    "    log_gradients_in_model(exp.model,\n",
    "                            summary_writer,\n",
    "                            epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bc822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all_np = np.array(preds_all)\n",
    "y_actual_all = np.array(y_actual_all)\n",
    "preds_all_np = preds_all_np.reshape(-1, preds_all_np.shape[-2], preds_all_np.shape[-1])\n",
    "y_actual_all = y_actual_all.reshape(-1, y_actual_all.shape[-2], y_actual_all.shape[-1])\n",
    "\n",
    "# Save results\n",
    "folder_path = './results/' + setting +'/'\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "np.save(folder_path + 'preds_all_train.npy', preds_all_np)\n",
    "np.save(folder_path + 'true_values_all_train.npy', y_actual_all)\n",
    "best_model_path = path + '/checkpoint_informer.pth'\n",
    "exp.model.load_state_dict(torch.load(best_model_path))\n",
    "exp.model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b2d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1742a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb82884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a025c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4471634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
